{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "pip install --user jupyter_contrib_nbextensions\n",
    "jupyter contrib nbextension install --user\n",
    "jupyter nbextension enable spellchecker/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is markdown with misspell words like wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nbspellcheck: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nbspellcheck 0_wbs.md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --user nbspellcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/home/jupyter-butzer/.local/bin/nbspellcheck.py\", line 170, in <module>\r\n",
      "    notebook = json.load(finput)\r\n",
      "  File \"/opt/tljh/user/lib/python3.7/json/__init__.py\", line 296, in load\r\n",
      "    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\r\n",
      "  File \"/opt/tljh/user/lib/python3.7/json/__init__.py\", line 348, in loads\r\n",
      "    return _default_decoder.decode(s)\r\n",
      "  File \"/opt/tljh/user/lib/python3.7/json/decoder.py\", line 337, in decode\r\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n",
      "  File \"/opt/tljh/user/lib/python3.7/json/decoder.py\", line 355, in raw_decode\r\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n"
     ]
    }
   ],
   "source": [
    "!~/.local/bin/nbspellcheck.py 0_wbs.md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__pycache__\tgeobuf\t\t\t\t   panel\r\n",
      "bokeh\t\tgeoviews\t\t\t   pinstance\r\n",
      "cm2html\t\tholoviews\t\t\t   pyan3\r\n",
      "cm2latex\thvplot\t\t\t\t   pybabel\r\n",
      "cm2man\t\tintake\t\t\t\t   pycc\r\n",
      "cm2pseudoxml\tintake-server\t\t\t   pyct\r\n",
      "cm2xetex\tjupyter-contrib\t\t\t   sphinx-apidoc\r\n",
      "cm2xml\t\tjupyter-contrib-nbextension\t   sphinx-autogen\r\n",
      "cmark\t\tjupyter-nbextensions_configurator  sphinx-build\r\n",
      "colorcet\tmarkdown_py\t\t\t   sphinx-quickstart\r\n",
      "dask-scheduler\tmercantile\t\t\t   supermercado\r\n",
      "dask-ssh\tnbspellcheck.py\t\t\t   tqdm\r\n",
      "dask-worker\tnltk\t\t\t\t   unidecode\r\n",
      "datashader\tnumba\t\t\t\t   wsdump.py\r\n"
     ]
    }
   ],
   "source": [
    "! ls ~/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'google', 'microsoft'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()  # loads default word frequency list\n",
    "#spell.word_frequency.load_text_file('./0_wbs.md')\n",
    "\n",
    "# if I just want to make sure some words are not flagged as misspelled\n",
    "spell.word_frequency.load_words(['microsoft', 'apple', 'google'])\n",
    "spell.known(['microsoft', 'google'])  # will return both now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SpellChecker in module spellchecker.spellchecker object:\n",
      "\n",
      "class SpellChecker(builtins.object)\n",
      " |  SpellChecker(language='en', local_dictionary=None, distance=2, tokenizer=None, case_sensitive=False)\n",
      " |  \n",
      " |  The SpellChecker class encapsulates the basics needed to accomplish a\n",
      " |  simple spell checking algorithm. It is based on the work by\n",
      " |  Peter Norvig (https://norvig.com/spell-correct.html)\n",
      " |  \n",
      " |  Args:\n",
      " |      language (str): The language of the dictionary to load or None             for no dictionary. Supported languages are `en`, `es`, `de`, `fr`,             `pt` and `ru`. Defaults to `en`\n",
      " |      local_dictionary (str): The path to a locally stored word             frequency dictionary; if provided, no language will be loaded\n",
      " |      distance (int): The edit distance to use. Defaults to 2.\n",
      " |      case_sensitive (bool): Flag to use a case sensitive dictionary or             not, only available when not using a language dictionary.\n",
      " |  Note:\n",
      " |      Using a case sensitive dictionary can be slow to correct words.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, key)\n",
      " |      setup easier known checks\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |      setup easier frequency checks\n",
      " |  \n",
      " |  __init__(self, language='en', local_dictionary=None, distance=2, tokenizer=None, case_sensitive=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      setup iter support\n",
      " |  \n",
      " |  candidates(self, word)\n",
      " |      Generate possible spelling corrections for the provided word up to\n",
      " |      an edit distance of two, if and only when needed\n",
      " |      \n",
      " |      Args:\n",
      " |          word (str): The word for which to calculate candidate spellings\n",
      " |      Returns:\n",
      " |          set: The set of words that are possible candidates\n",
      " |  \n",
      " |  correction(self, word)\n",
      " |      The most probable correct spelling for the word\n",
      " |      \n",
      " |      Args:\n",
      " |          word (str): The word to correct\n",
      " |      Returns:\n",
      " |          str: The most likely candidate\n",
      " |  \n",
      " |  edit_distance_1(self, word)\n",
      " |      Compute all strings that are one edit away from `word` using only\n",
      " |      the letters in the corpus\n",
      " |      \n",
      " |      Args:\n",
      " |          word (str): The word for which to calculate the edit distance\n",
      " |      Returns:\n",
      " |          set: The set of strings that are edit distance one from the                 provided word\n",
      " |  \n",
      " |  edit_distance_2(self, word)\n",
      " |      Compute all strings that are two edits away from `word` using only\n",
      " |      the letters in the corpus\n",
      " |      \n",
      " |      Args:\n",
      " |          word (str): The word for which to calculate the edit distance\n",
      " |      Returns:\n",
      " |          set: The set of strings that are edit distance two from the                 provided word\n",
      " |  \n",
      " |  export(self, filepath, encoding='utf-8', gzipped=True)\n",
      " |      Export the word frequency list for import in the future\n",
      " |      \n",
      " |      Args:\n",
      " |         filepath (str): The filepath to the exported dictionary\n",
      " |         encoding (str): The encoding of the resulting output\n",
      " |         gzipped (bool): Whether to gzip the dictionary or not\n",
      " |  \n",
      " |  known(self, words)\n",
      " |      The subset of `words` that appear in the dictionary of words\n",
      " |      \n",
      " |      Args:\n",
      " |          words (list): List of words to determine which are in the                 corpus\n",
      " |      Returns:\n",
      " |          set: The set of those words from the input that are in the                 corpus\n",
      " |  \n",
      " |  split_words(self, text)\n",
      " |      Split text into individual `words` using either a simple whitespace\n",
      " |      regex or the passed in tokenizer\n",
      " |      \n",
      " |      Args:\n",
      " |          text (str): The text to split into individual words\n",
      " |      Returns:\n",
      " |          list(str): A listing of all words in the provided text\n",
      " |  \n",
      " |  unknown(self, words)\n",
      " |      The subset of `words` that do not appear in the dictionary\n",
      " |      \n",
      " |      Args:\n",
      " |          words (list): List of words to determine which are not in the                 corpus\n",
      " |      Returns:\n",
      " |          set: The set of those words from the input that are not in                 the corpus\n",
      " |  \n",
      " |  word_probability(self, word, total_words=None)\n",
      " |      Calculate the frequency to the `word` provided as seen across the\n",
      " |      entire dictionary; function was a misnomar and is therefore\n",
      " |      deprecated!\n",
      " |      \n",
      " |      Args:\n",
      " |          word (str): The word for which the word probability is                 calculated\n",
      " |          total_words (int): The total number of words to use in the                 calculation; use the default for using the whole word                 frequency\n",
      " |      Returns:\n",
      " |          float: The probability that the word is the correct word\n",
      " |      Note:\n",
      " |          Deprecated as of version 0.6.1; use `word_usage_frequency`                 instead\n",
      " |      Note:\n",
      " |          Will be removed in version 0.6.3\n",
      " |  \n",
      " |  word_usage_frequency(self, word, total_words=None)\n",
      " |      Calculate the frequency to the `word` provided as seen across the\n",
      " |      entire dictionary\n",
      " |      \n",
      " |      Args:\n",
      " |          word (str): The word for which the word probability is                 calculated\n",
      " |          total_words (int): The total number of words to use in the                 calculation; use the default for using the whole word                 frequency\n",
      " |      Returns:\n",
      " |          float: The probability that the word is the correct word\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  distance\n",
      " |      int: The maximum edit distance to calculate\n",
      " |      \n",
      " |      Note:\n",
      " |          Valid values are 1 or 2; if an invalid value is passed,                 defaults to 2\n",
      " |  \n",
      " |  word_frequency\n",
      " |      WordFrequency: An encapsulation of the word frequency `dictionary`\n",
      " |      \n",
      " |      Note:\n",
      " |          Not settable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spelling'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.correction('spellin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def words(text): return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words = words(open('0_wbs.md').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['high',\n",
       " 'level',\n",
       " 'wbs',\n",
       " 'us',\n",
       " 'conus',\n",
       " '20',\n",
       " 'years',\n",
       " 'by',\n",
       " 'june',\n",
       " 's3',\n",
       " 'data',\n",
       " 'organization',\n",
       " 'and',\n",
       " 'conus',\n",
       " 'inputs',\n",
       " 'steffi',\n",
       " 'and',\n",
       " 'olena',\n",
       " 'to',\n",
       " 'get',\n",
       " 'input',\n",
       " 'data',\n",
       " 'ready',\n",
       " 'for',\n",
       " 'conus',\n",
       " 'cleanup',\n",
       " 'all',\n",
       " 'old',\n",
       " 's3',\n",
       " 'bucket',\n",
       " 'data',\n",
       " 'push',\n",
       " 'to',\n",
       " 'netapp',\n",
       " 'if',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'this',\n",
       " 'data',\n",
       " 'use',\n",
       " 'the',\n",
       " 'vdi',\n",
       " 'and',\n",
       " 'scp',\n",
       " 'and',\n",
       " 's3fs',\n",
       " 'mounts',\n",
       " 'learn',\n",
       " 'aws',\n",
       " 's3',\n",
       " 'rm',\n",
       " 'recursive',\n",
       " 'dryrun',\n",
       " 'learn',\n",
       " 'to',\n",
       " 'use',\n",
       " 'tmux',\n",
       " 'setup',\n",
       " 'ubuntu',\n",
       " 'ssh',\n",
       " 'keys',\n",
       " 'for',\n",
       " 'olena',\n",
       " 'code',\n",
       " 'preparation',\n",
       " 'and',\n",
       " 'refinements',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'gridmeister',\n",
       " 'sprawl',\n",
       " 'clone',\n",
       " 'the',\n",
       " 'repo',\n",
       " 'and',\n",
       " 'organize',\n",
       " 'the',\n",
       " 'code',\n",
       " 'run',\n",
       " 'the',\n",
       " 'model',\n",
       " 'for',\n",
       " '3',\n",
       " 'years',\n",
       " 'one',\n",
       " 'tile',\n",
       " 'wait',\n",
       " 'for',\n",
       " 'data',\n",
       " 'wrangle',\n",
       " 'and',\n",
       " 'data',\n",
       " 'cleanup',\n",
       " 'activities',\n",
       " 'refine',\n",
       " 'logging',\n",
       " 'refine',\n",
       " 'orchestration',\n",
       " 'build',\n",
       " 'tile',\n",
       " 'tracker',\n",
       " 'run',\n",
       " 'path_param',\n",
       " 'yml',\n",
       " 'extent',\n",
       " 'analyzer',\n",
       " 'notebook',\n",
       " 'find',\n",
       " 'this',\n",
       " 'notebook',\n",
       " 'and',\n",
       " 'organize',\n",
       " 'into',\n",
       " 'conus',\n",
       " 'veget',\n",
       " 'repo',\n",
       " 'code',\n",
       " 'delivery',\n",
       " 'and',\n",
       " 'handoff',\n",
       " 'back',\n",
       " 'to',\n",
       " 'stef',\n",
       " 'data',\n",
       " 'push',\n",
       " 'to',\n",
       " 'netapp',\n",
       " 'and',\n",
       " 'or',\n",
       " 'denali',\n",
       " 'devops',\n",
       " 'tony',\n",
       " 'transition',\n",
       " 'to',\n",
       " 'compositing',\n",
       " 'and',\n",
       " 'lcmap',\n",
       " 'projects',\n",
       " 'project',\n",
       " 'completion',\n",
       " 'of',\n",
       " 'landsat',\n",
       " 'funded',\n",
       " 'et',\n",
       " 'new',\n",
       " 'wird',\n",
       " 'here',\n",
       " 'mispel',\n",
       " 'contaner']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aws',\n",
       " 'contaner',\n",
       " 'conus',\n",
       " 'devops',\n",
       " 'dryrun',\n",
       " 'gridmeister',\n",
       " 'lcmap',\n",
       " 'mispel',\n",
       " 'netapp',\n",
       " 'olena',\n",
       " 'path_param',\n",
       " 'rm',\n",
       " 's3',\n",
       " 's3fs',\n",
       " 'scp',\n",
       " 'ssh',\n",
       " 'steffi',\n",
       " 'tmux',\n",
       " 'ubuntu',\n",
       " 'vdi',\n",
       " 'veget',\n",
       " 'wbs',\n",
       " 'yml'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.unknown(my_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dispel'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.correction('mispel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook simple_md.ipynb to rst\n",
      "[NbConvertApp] Writing 246 bytes to simple_md.rst\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to rst simple_md.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a simple md notebook\r\n",
      "============================\r\n",
      "\r\n",
      "-  here be text\r\n",
      "\r\n",
      "-  this is a picture\r\n",
      "\r\n",
      ".. figure:: https://media2.giphy.com/media/3o6nV0vQCWaUVCOecM/200.webp?cid=ecf05e47y5c08hh685pgu9u286mmyvlqq6v0vy6k0lrxmfhk&rid=200.webp\r\n",
      "   :alt: \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat simple_md.rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook simple_md.ipynb to markdown\n",
      "[NbConvertApp] Writing 218 bytes to simple_md.md\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to markdown simple_md.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#  This is a simple md notebook\n",
       "\n",
       "- here be text\n",
       "\n",
       "- this is a picture \n",
       "\n",
       "![](https://media2.giphy.com/media/3o6nV0vQCWaUVCOecM/200.webp?cid=ecf05e47y5c08hh685pgu9u286mmyvlqq6v0vy6k0lrxmfhk&rid=200.webp)\n",
       "\n",
       "\n",
       "```python\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "with open('simple_md.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# so the pipeline\n",
    "\n",
    "1. edits simple_md.ipynb\n",
    "2. convert to markdown single or in batch - makefile?\n",
    "3. rm original or always edit original\n",
    "4. or skip the conversion all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
